# STD-Based QC System: Advanced Technical Enhancements

**ì‘ì„±ì¼**: 2025-12-17
**ìƒíƒœ**: ê¸°ìˆ ì  ë³´ì™„ ê³„íš (Technical Enhancement Specification)
**ëª©ì **: STD ê¸°ë°˜ í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ë° ì•„í‚¤í…ì²˜ ê³ ë„í™”

---

## ğŸ“‹ Executive Summary

ê¸°ì¡´ `STD_BASED_QC_SYSTEM_PLAN.md`ëŠ” **ê¸°ìˆ ì ìœ¼ë¡œ ì¶©ë¶„íˆ ê°€ëŠ¥í•œ ìƒíƒœ**ì…ë‹ˆë‹¤. ë³¸ ë¬¸ì„œëŠ” í•´ë‹¹ ê³„íšì„ í•œ ë‹¨ê³„ ë” ë°œì „ì‹œí‚¤ê¸° ìœ„í•œ **7ê°€ì§€ í•µì‹¬ ê¸°ìˆ  ë³´ê°• ì‚¬í•­**ì„ ìƒì„¸íˆ ì •ì˜í•©ë‹ˆë‹¤.

### í•µì‹¬ ê°œì„  ì˜ì—­
1. **STD Statistical Model**: ë‹¨ì¼ ì´ë¯¸ì§€ â†’ í†µê³„ì  ë¶„í¬ ëª¨ë¸ (mean Â± Ïƒ)
2. **Elastic Alignment**: êµ¬ì¡° ë¹„êµ ì „ ì •ë ¬ ë‹¨ê³„ (Anchor Zone ê¸°ë°˜)
3. **Worst-Case Color Metrics**: í‰ê·  Î”E ì™¸ ê·¹ê°’ ì§€í‘œ ë„ì…
4. **Ink-Aware Comparison**: Zoneê³¼ Ink ì´ì¤‘ ê´€ì  ë¹„êµ ë ˆì´ì–´
5. **Explainability Layer**: NG ì›ì¸ ìë™ ì„¤ëª… ìƒì„±
6. **Performance & Stability**: ìºì‹±, Fail-Safe ì„¤ê³„, Confidence Score ë¶„ë¦¬
7. **Phenomenological Classification**: ê²°í•¨ ìœ í˜• ë¶„ë¥˜ ì²´ê³„

---

## 1. STD Statistical Model (í†µê³„ì  í‘œì¤€ ëª¨ë¸)

### 1.1 í˜„ì¬ ì„¤ê³„ì˜ í•œê³„
- **ë¬¸ì œ**: STDë¥¼ "ì¢‹ì€ í•œ ì¥ì˜ ì´ë¯¸ì§€"ë¡œ ì·¨ê¸‰
- **ë¦¬ìŠ¤í¬**:
  - STD ìì²´ì˜ ìì—° í¸ì°¨(natural variance)ë¥¼ ë¬´ì‹œ
  - ì–‘ì‚° ìƒ˜í”Œ ë¹„êµ ì‹œ False Negative ë°œìƒ (ì‹¤ì œë¡œëŠ” OKì¸ë° NGë¡œ íŒì •)
  - ìƒí•œ/ì¤‘ê°„/í•˜í•œ ì„¤ì • ì‹œ ê·¼ê±° ë¶€ì¡±

### 1.2 ê°œì„  ë°©ì•ˆ: í†µê³„ì  ë¶„í¬ ëª¨ë¸

#### ë°ì´í„° ìˆ˜ì§‘
```
STD Samples: n = 5~10ì¥ (ë™ì¼ SKU, ë™ì¼ ìƒì‚° ë°°ì¹˜)
ê° ìƒ˜í”Œì— ëŒ€í•´:
  - Radial Profile (500 points)
  - Zone Results (A, B, C zones)
  - Ink Colors (Lab ìƒ‰ìƒ)
  - Zone Boundaries (transitions)
```

#### í†µê³„ ëª¨ë¸ êµ¬ì„±
```python
class STDStatisticalModel:
    def __init__(self, sku_code: str, samples: List[AnalysisResult]):
        self.sku_code = sku_code
        self.n_samples = len(samples)

        # 1. Radial Profile í†µê³„
        self.profile_mean = np.mean([s.radial_profile for s in samples], axis=0)
        self.profile_std = np.std([s.radial_profile for s in samples], axis=0)
        self.profile_covariance = np.cov([s.radial_profile for s in samples])

        # 2. Zone Color í†µê³„ (ê° Zoneë³„)
        for zone_name in ['A', 'B', 'C']:
            colors = [s.zones[zone_name].color_lab for s in samples]
            self.zones[zone_name] = {
                'mean_L': np.mean([c[0] for c in colors]),
                'std_L': np.std([c[0] for c in colors]),
                'mean_a': np.mean([c[1] for c in colors]),
                'std_a': np.std([c[1] for c in colors]),
                'mean_b': np.mean([c[2] for c in colors]),
                'std_b': np.std([c[2] for c in colors]),
                'covariance': np.cov(colors)
            }

        # 3. Zone Boundary í†µê³„
        for transition in ['inner_to_A', 'A_to_B', 'B_to_C']:
            positions = [s.boundaries[transition] for s in samples]
            self.boundaries[transition] = {
                'mean': np.mean(positions),
                'std': np.std(positions),
                'percentile_5': np.percentile(positions, 5),
                'percentile_95': np.percentile(positions, 95)
            }

        # 4. Ink Count í†µê³„ (Image-Based)
        ink_counts = [s.ink_analysis.n_inks for s in samples]
        self.ink_stats = {
            'mode': stats.mode(ink_counts)[0],  # ìµœë¹ˆê°’
            'allowed_values': set(ink_counts)  # í—ˆìš© ë²”ìœ„
        }
```

#### íŒì • ê¸°ì¤€ ìë™ ë„ì¶œ
```python
def derive_acceptance_criteria(self, confidence_level=0.95):
    """STD ë¶„í¬ë¡œë¶€í„° í•©ê²© ê¸°ì¤€ ìë™ ë„ì¶œ"""
    z_score = stats.norm.ppf((1 + confidence_level) / 2)  # 95% â†’ z=1.96

    for zone_name in ['A', 'B', 'C']:
        zone_stats = self.zones[zone_name]

        # Î”E í—ˆìš© ë²”ìœ„ = 3 Ã— std (99.7% í¬í•¨)
        self.acceptance_criteria[zone_name] = {
            'max_delta_E': 3.0 * np.sqrt(
                zone_stats['std_L']**2 +
                zone_stats['std_a']**2 +
                zone_stats['std_b']**2
            ),
            'L_range': (
                zone_stats['mean_L'] - z_score * zone_stats['std_L'],
                zone_stats['mean_L'] + z_score * zone_stats['std_L']
            )
        }

    # Boundary í—ˆìš© ì˜¤ì°¨ = 2 Ã— std
    for transition, stats_dict in self.boundaries.items():
        self.acceptance_criteria[transition] = {
            'tolerance': 2.0 * stats_dict['std'],
            'range': (stats_dict['percentile_5'], stats_dict['percentile_95'])
        }
```

### 1.3 DB Schema ë³€ê²½

#### ê¸°ì¡´ (Single STD)
```sql
std_profiles (
    id, sku_code, version, profile_data JSON, created_at
)
```

#### ê°œì„  (Statistical STD)
```sql
-- STD ë©”íƒ€ë°ì´í„°
std_models (
    id SERIAL PRIMARY KEY,
    sku_code VARCHAR(50) NOT NULL,
    version VARCHAR(20) NOT NULL,
    n_samples INTEGER NOT NULL,  -- 5~10
    created_at TIMESTAMP,
    approved_by VARCHAR(100),
    approved_at TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE
);

-- STD êµ¬ì„± ìƒ˜í”Œë“¤ (ê°œë³„ ì´ë¯¸ì§€)
std_samples (
    id SERIAL PRIMARY KEY,
    std_model_id INTEGER REFERENCES std_models(id),
    sample_index INTEGER,  -- 1~n
    image_path VARCHAR(500),
    analysis_result JSONB,  -- ì „ì²´ ë¶„ì„ ê²°ê³¼
    created_at TIMESTAMP
);

-- STD í†µê³„ ìš”ì•½ (ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™”)
std_statistics (
    id SERIAL PRIMARY KEY,
    std_model_id INTEGER REFERENCES std_models(id),
    zone_name VARCHAR(10),  -- 'A', 'B', 'C', 'profile', 'boundary_AB'

    -- ìƒ‰ìƒ í†µê³„
    mean_L FLOAT,
    std_L FLOAT,
    mean_a FLOAT,
    std_a FLOAT,
    mean_b FLOAT,
    std_b FLOAT,

    -- êµ¬ì¡° í†µê³„
    mean_position FLOAT,  -- Boundary ìœ„ì¹˜
    std_position FLOAT,
    percentile_5 FLOAT,
    percentile_95 FLOAT,

    -- ìƒì„¸ í†µê³„ (covariance matrix ë“±)
    detailed_stats JSONB
);

CREATE INDEX idx_std_stats_model ON std_statistics(std_model_id, zone_name);
```

---

## 2. Elastic Alignment (íƒ„ì„± ì •ë ¬)

### 2.1 ë¬¸ì œ ì •ì˜
- **êµ¬ì¡°ì  ë³€ë™**: ë‚´ì¸¡ ë§ ìœ„ì¹˜ ë³€í™” (Â±5%), ì™¸ì¸¡ ë§ ì´ë™, ì¤‘ì‹¬ ì¹˜ìš°ì¹¨
- **í˜„ì¬ ë°©ì‹**: ê³ ì • ìœ„ì¹˜ ë¹„êµ â†’ íŒ¨í„´ shiftë¡œ ì¸í•œ False Negative
- **ì˜ˆì‹œ**: Zone Bê°€ 2mm ì™¸ê³½ìœ¼ë¡œ ì´ë™í–ˆì§€ë§Œ êµ¬ì¡° ìì²´ëŠ” ë™ì¼í•œ ê²½ìš°

### 2.2 Anchor Zone ê¸°ë°˜ ì •ë ¬ ì „ëµ

#### Phase 1: Anchor Zone ì‹ë³„
```python
def identify_anchor_zone(test_profile, std_profile):
    """ê°€ì¥ ì•ˆì •ì ì¸ Zoneì„ Anchorë¡œ ì„ íƒ"""
    # C Zone (ìµœì™¸ê³½)ì„ ê¸°ë³¸ Anchorë¡œ ì‚¬ìš©
    # ì´ìœ : ë Œì¦ˆ ê°€ì¥ìë¦¬ëŠ” ë¬¼ë¦¬ì ìœ¼ë¡œ ê°€ì¥ ì•ˆì •ì 
    anchor_zone = 'C'

    # Anchor Zoneì˜ ì¤‘ì‹¬ ìœ„ì¹˜ ì¶”ì¶œ
    test_anchor_center = test_profile.zones[anchor_zone].center_position
    std_anchor_center = std_profile.zones[anchor_zone].center_position

    # ìœ„ì¹˜ ì°¨ì´ ê³„ì‚°
    shift = test_anchor_center - std_anchor_center
    return shift, anchor_zone
```

#### Phase 2: Circular Shift Alignment
```python
def align_radial_profiles(test_profile, std_profile, shift):
    """Radial Profileì„ shiftë§Œí¼ ìˆœí™˜ ì´ë™"""
    # shift > 0: ì™¸ê³½ ë°©í–¥ ì´ë™
    # shift < 0: ë‚´ì¸¡ ë°©í–¥ ì´ë™

    shift_pixels = int(shift / pixel_size)  # mm â†’ pixel

    if shift_pixels > 0:
        # ìš°ì¸¡ ìˆœí™˜ ì´ë™
        aligned_test = np.roll(test_profile, shift_pixels)
    else:
        # ì¢Œì¸¡ ìˆœí™˜ ì´ë™
        aligned_test = np.roll(test_profile, shift_pixels)

    return aligned_test
```

#### Phase 3: DTW Pre-Alignment (Fine-Tuning)
```python
from dtaidistance import dtw

def fine_tune_alignment(test_profile, std_profile, max_warp=10):
    """DTWë¡œ ë¯¸ì„¸ ì •ë ¬ (Â±10 pixel ë²”ìœ„ ë‚´)"""
    # DTW path ê³„ì‚°
    path = dtw.warping_path(test_profile, std_profile)

    # Warp ì œì•½: ìµœëŒ€ Â±10 pixel ì´ë‚´ë§Œ í—ˆìš©
    # (ê³¼ë„í•œ warping ë°©ì§€)
    constrained_path = constrain_warping(path, max_warp=10)

    # Path ì ìš©í•˜ì—¬ ì •ë ¬ëœ í”„ë¡œíŒŒì¼ ìƒì„±
    aligned_test = apply_warping_path(test_profile, constrained_path)

    return aligned_test, constrained_path
```

#### Phase 4: Zone Boundary Re-Detection
```python
def realign_zone_boundaries(aligned_profile, std_boundaries):
    """ì •ë ¬ëœ í”„ë¡œíŒŒì¼ì—ì„œ Zone ê²½ê³„ ì¬íƒì§€"""
    # STD ê²½ê³„ ìœ„ì¹˜ ì£¼ë³€ Â±20 pixel ë²”ìœ„ì—ì„œ íƒìƒ‰
    detected_boundaries = {}

    for boundary_name, std_position in std_boundaries.items():
        search_window = (std_position - 20, std_position + 20)

        # ê²½ê³„ íƒì§€ (Gradient ê¸°ë°˜)
        gradient = np.gradient(aligned_profile)
        peak_idx = np.argmax(np.abs(gradient[search_window[0]:search_window[1]]))

        detected_boundaries[boundary_name] = search_window[0] + peak_idx

    return detected_boundaries
```

### 2.3 Alignment Quality Metrics
```python
def evaluate_alignment_quality(aligned_test, std_profile):
    """ì •ë ¬ í’ˆì§ˆ í‰ê°€"""
    # 1. Correlation Coefficient
    corr = np.corrcoef(aligned_test, std_profile)[0, 1]

    # 2. RMSE (Root Mean Square Error)
    rmse = np.sqrt(np.mean((aligned_test - std_profile)**2))

    # 3. Structural Similarity Index (SSIM)
    from skimage.metrics import structural_similarity
    ssim = structural_similarity(aligned_test, std_profile)

    alignment_quality = {
        'correlation': corr,  # 0.95 ì´ìƒ ì–‘í˜¸
        'rmse': rmse,  # 5.0 ì´í•˜ ì–‘í˜¸
        'ssim': ssim,  # 0.90 ì´ìƒ ì–‘í˜¸
        'is_good': corr > 0.95 and rmse < 5.0 and ssim > 0.90
    }

    return alignment_quality
```

---

## 3. Worst-Case Color Metrics (ê·¹ê°’ ìƒ‰ìƒ ì§€í‘œ)

### 3.1 í‰ê·  Î”Eì˜ í•œê³„
- **ë¬¸ì œ**: í‰ê·  Î”E = 2.5 (OK)ì¸ë° ì‹œê°ì ìœ¼ë¡œ NG
- **ì›ì¸**: êµ­ì†Œì  ê²°í•¨ (5% ì˜ì—­ì´ Î”E=15.0)ì´ í‰ê· ì— ë¬»í˜

### 3.2 ë³´ì™„ ì§€í‘œ ì²´ê³„

#### 3.2.1 Percentile-Based Î”E
```python
def calculate_percentile_delta_e(test_zone, std_zone):
    """í”½ì…€ë³„ Î”E ë¶„í¬ì˜ ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°"""
    from colormath.color_diff import delta_e_cie2000

    # ê° í”½ì…€ë³„ Î”E ê³„ì‚°
    pixel_delta_e = []
    for test_lab, std_lab in zip(test_zone.pixels_lab, std_zone.pixels_lab):
        de = delta_e_cie2000(test_lab, std_lab)
        pixel_delta_e.append(de)

    pixel_delta_e = np.array(pixel_delta_e)

    return {
        'mean': np.mean(pixel_delta_e),
        'median': np.median(pixel_delta_e),
        'p75': np.percentile(pixel_delta_e, 75),
        'p90': np.percentile(pixel_delta_e, 90),
        'p95': np.percentile(pixel_delta_e, 95),  # â­ í•µì‹¬ ì§€í‘œ
        'p99': np.percentile(pixel_delta_e, 99),
        'max': np.max(pixel_delta_e)
    }
```

#### 3.2.2 Spatial Hotspot Detection
```python
def detect_color_hotspots(test_image_lab, std_image_lab, threshold=10.0, min_area=100):
    """Î”E ì´ìƒ ì˜ì—­(Hotspot) íƒì§€"""
    from skimage.measure import label, regionprops

    # í”½ì…€ë³„ Î”E ë§µ ìƒì„±
    delta_e_map = np.zeros(test_image_lab.shape[:2])
    for i in range(test_image_lab.shape[0]):
        for j in range(test_image_lab.shape[1]):
            delta_e_map[i, j] = delta_e_cie2000(
                test_image_lab[i, j],
                std_image_lab[i, j]
            )

    # Hotspot ë§ˆìŠ¤í¬ (Î”E > 10.0)
    hotspot_mask = delta_e_map > threshold

    # Connected Component ë¶„ì„
    labeled_mask = label(hotspot_mask)
    regions = regionprops(labeled_mask)

    hotspots = []
    for region in regions:
        if region.area >= min_area:  # ìµœì†Œ 100 í”½ì…€ ì´ìƒ
            hotspots.append({
                'area': region.area,
                'centroid': region.centroid,
                'bbox': region.bbox,
                'mean_delta_e': np.mean(delta_e_map[labeled_mask == region.label]),
                'max_delta_e': np.max(delta_e_map[labeled_mask == region.label])
            })

    # Severity ìˆœìœ¼ë¡œ ì •ë ¬
    hotspots.sort(key=lambda x: x['mean_delta_e'] * x['area'], reverse=True)

    return hotspots, delta_e_map
```

#### 3.2.3 Spatial Clustering (DBSCAN)
```python
from sklearn.cluster import DBSCAN

def cluster_abnormal_pixels(delta_e_map, threshold=8.0):
    """Î”E ì´ìƒ í”½ì…€ì˜ ê³µê°„ì  êµ°ì§‘ ë¶„ì„"""
    # ì´ìƒ í”½ì…€ ì¢Œí‘œ ì¶”ì¶œ
    abnormal_coords = np.argwhere(delta_e_map > threshold)

    if len(abnormal_coords) < 10:
        return []  # ì´ìƒ í”½ì…€ ë„ˆë¬´ ì ìŒ

    # DBSCAN í´ëŸ¬ìŠ¤í„°ë§ (eps=5 í”½ì…€)
    clustering = DBSCAN(eps=5, min_samples=10).fit(abnormal_coords)

    clusters = []
    for cluster_id in set(clustering.labels_):
        if cluster_id == -1:  # Noise
            continue

        cluster_mask = clustering.labels_ == cluster_id
        cluster_coords = abnormal_coords[cluster_mask]

        clusters.append({
            'n_pixels': len(cluster_coords),
            'center': np.mean(cluster_coords, axis=0),
            'radius': np.std(cluster_coords, axis=0),
            'mean_delta_e': np.mean(delta_e_map[cluster_coords[:, 0], cluster_coords[:, 1]])
        })

    return clusters
```

### 3.3 íŒì • ê¸°ì¤€ í†µí•©
```python
def evaluate_color_quality(test_zone, std_zone, criteria):
    """ìƒ‰ìƒ í’ˆì§ˆ ì¢…í•© í‰ê°€"""
    percentiles = calculate_percentile_delta_e(test_zone, std_zone)
    hotspots, delta_e_map = detect_color_hotspots(test_zone.image, std_zone.image)
    clusters = cluster_abnormal_pixels(delta_e_map)

    # íŒì • ë¡œì§
    is_pass = True
    fail_reasons = []

    # 1. í‰ê·  Î”E ì²´í¬
    if percentiles['mean'] > criteria['max_mean_delta_e']:
        is_pass = False
        fail_reasons.append(f"í‰ê·  Î”E={percentiles['mean']:.2f} (ê¸°ì¤€: {criteria['max_mean_delta_e']})")

    # 2. 95 ë°±ë¶„ìœ„ìˆ˜ ì²´í¬ (â­ í•µì‹¬)
    if percentiles['p95'] > criteria['max_p95_delta_e']:
        is_pass = False
        fail_reasons.append(f"95% Î”E={percentiles['p95']:.2f} (ê¸°ì¤€: {criteria['max_p95_delta_e']})")

    # 3. Hotspot ì²´í¬
    if len(hotspots) > 0:
        largest_hotspot = hotspots[0]
        if largest_hotspot['area'] > criteria['max_hotspot_area']:
            is_pass = False
            fail_reasons.append(
                f"Hotspot ë°œê²¬: ë©´ì ={largest_hotspot['area']}px, "
                f"í‰ê·  Î”E={largest_hotspot['mean_delta_e']:.2f}"
            )

    # 4. í´ëŸ¬ìŠ¤í„° ì²´í¬
    if len(clusters) > criteria['max_allowed_clusters']:
        is_pass = False
        fail_reasons.append(f"ì´ìƒ ì˜ì—­ {len(clusters)}ê°œ ë°œê²¬ (ê¸°ì¤€: {criteria['max_allowed_clusters']})")

    return {
        'is_pass': is_pass,
        'fail_reasons': fail_reasons,
        'percentiles': percentiles,
        'hotspots': hotspots,
        'clusters': clusters,
        'delta_e_map': delta_e_map  # ì‹œê°í™”ìš©
    }
```

---

## 4. Ink-Aware Comparison Layer (ì‰í¬ ì¸ì‹ ë¹„êµ)

### 4.1 ë¬¸ì œ ì •ì˜
- **Zone â‰  Ink**: Zone Bì— ì‰í¬ 2ê°œ (Blue, Sky Blue) ì¡´ì¬
- **í˜„ì¬ í•œê³„**: Zone í‰ê·  ìƒ‰ìƒë§Œ ë¹„êµ â†’ ì‰í¬ í˜¼í•©/ëˆ„ë½ ê°ì§€ ë¶ˆê°€

### 4.2 Dual Scoring Architecture

#### 4.2.1 Zone-Based Score (êµ¬ì¡°ì  ìœ ì‚¬ë„)
```python
def calculate_zone_based_score(test_zones, std_zones):
    """Zone ë‹¨ìœ„ êµ¬ì¡° ë¹„êµ"""
    zone_scores = {}

    for zone_name in ['A', 'B', 'C']:
        test_zone = test_zones[zone_name]
        std_zone = std_zones[zone_name]

        # 1. Zone ìƒ‰ìƒ ìœ ì‚¬ë„ (í‰ê·  Lab)
        color_score = 100 - delta_e_cie2000(
            test_zone.mean_lab,
            std_zone.mean_lab
        ) * 10  # Î”E=1 â†’ -10ì 

        # 2. Zone ê²½ê³„ ìœ„ì¹˜ ìœ ì‚¬ë„
        boundary_diff = abs(test_zone.boundary - std_zone.boundary)
        boundary_score = 100 - boundary_diff * 20  # 1px ì°¨ì´ â†’ -20ì 

        # 3. Zone ë©´ì  ë¹„ìœ¨ ìœ ì‚¬ë„
        area_ratio_diff = abs(test_zone.area_ratio - std_zone.area_ratio)
        area_score = 100 - area_ratio_diff * 100  # 1% ì°¨ì´ â†’ -100ì 

        zone_scores[zone_name] = {
            'color': max(0, color_score),
            'boundary': max(0, boundary_score),
            'area': max(0, area_score),
            'total': (color_score + boundary_score + area_score) / 3
        }

    overall = np.mean([z['total'] for z in zone_scores.values()])
    return overall, zone_scores
```

#### 4.2.2 Ink-Based Score (ì‰í¬ ìœ ì‚¬ë„)
```python
def calculate_ink_based_score(test_inks, std_inks):
    """Ink ë‹¨ìœ„ ìƒ‰ìƒ ë¹„êµ"""
    # 1. Ink ê°œìˆ˜ ì¼ì¹˜ í™•ì¸
    if test_inks.n_inks != std_inks.n_inks:
        count_penalty = abs(test_inks.n_inks - std_inks.n_inks) * 30
        # 1ê°œ ì°¨ì´ â†’ -30ì 
    else:
        count_penalty = 0

    # 2. Ink ìƒ‰ìƒ ë§¤ì¹­ (Hungarian Algorithm)
    from scipy.optimize import linear_sum_assignment

    # Cost Matrix: ê° Ink ìŒì˜ Î”E
    cost_matrix = np.zeros((test_inks.n_inks, std_inks.n_inks))
    for i, test_ink in enumerate(test_inks.colors_lab):
        for j, std_ink in enumerate(std_inks.colors_lab):
            cost_matrix[i, j] = delta_e_cie2000(test_ink, std_ink)

    # ìµœì  ë§¤ì¹­
    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    matched_delta_e = cost_matrix[row_ind, col_ind]

    # 3. ìƒ‰ìƒ ì ìˆ˜ ê³„ì‚°
    color_scores = [100 - de * 10 for de in matched_delta_e]
    avg_color_score = np.mean(color_scores)

    # 4. ì¢…í•© ì ìˆ˜
    final_score = max(0, avg_color_score - count_penalty)

    return final_score, {
        'count_penalty': count_penalty,
        'matched_delta_e': matched_delta_e.tolist(),
        'color_scores': color_scores,
        'matching': list(zip(row_ind, col_ind))
    }
```

#### 4.2.3 Combined Scoring
```python
def calculate_combined_score(test_result, std_result, weights=None):
    """Zone-Based + Ink-Based í†µí•© ì ìˆ˜"""
    if weights is None:
        weights = {'zone': 0.5, 'ink': 0.5}  # ê¸°ë³¸ 50:50

    # Zone-Based Score
    zone_score, zone_details = calculate_zone_based_score(
        test_result.zones,
        std_result.zones
    )

    # Ink-Based Score
    ink_score, ink_details = calculate_ink_based_score(
        test_result.ink_analysis,
        std_result.ink_analysis
    )

    # ê°€ì¤‘ í‰ê· 
    combined_score = (
        zone_score * weights['zone'] +
        ink_score * weights['ink']
    )

    return {
        'total_score': combined_score,
        'zone_score': zone_score,
        'ink_score': ink_score,
        'zone_details': zone_details,
        'ink_details': ink_details,
        'weights': weights
    }
```

### 4.3 Diagnosis Matrix
```python
def diagnose_failure_type(zone_score, ink_score, threshold=70):
    """ì‹¤íŒ¨ ìœ í˜• ì§„ë‹¨"""
    if zone_score >= threshold and ink_score >= threshold:
        return 'PASS', 'OK'

    if zone_score < threshold and ink_score < threshold:
        return 'FAIL', 'êµ¬ì¡° ë° ì‰í¬ ëª¨ë‘ ì´ìƒ'

    if zone_score < threshold and ink_score >= threshold:
        return 'FAIL', 'êµ¬ì¡° ì´ìƒ (Zone ê²½ê³„/ë©´ì  ë¬¸ì œ), ì‰í¬ ìƒ‰ìƒì€ OK'

    if zone_score >= threshold and ink_score < threshold:
        return 'FAIL', 'êµ¬ì¡°ëŠ” OK, ì‰í¬ ìƒ‰ìƒ/ê°œìˆ˜ ì´ìƒ'

    return 'UNKNOWN', 'íŒì • ë¶ˆê°€'
```

---

## 5. Explainability Layer (ì„¤ëª… ê°€ëŠ¥ì„±)

### 5.1 ëª©í‘œ
- **What**: "FAIL ì ìˆ˜ 65ì " â†’ "ì™œ ì‹¤íŒ¨í–ˆëŠ”ê°€?"
- **Why**: í˜„ì¥ ì‘ì—…ìê°€ ì¡°ì¹˜ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì²´ì  ê·¼ê±° ì œì‹œ
- **How**: Top 3 ì‹¤íŒ¨ ì›ì¸ ìë™ ì¶”ì¶œ

### 5.2 Failure Reason Extraction

#### 5.2.1 ì›ì¸ í›„ë³´ ìˆ˜ì§‘
```python
class FailureReasonExtractor:
    def __init__(self):
        self.reasons = []

    def extract_all_reasons(self, test_result, std_result, comparison_result):
        """ëª¨ë“  ì‹¤íŒ¨ ì›ì¸ í›„ë³´ ì¶”ì¶œ"""
        self.reasons = []

        # 1. Zone ê²½ê³„ ì´íƒˆ
        for zone_name, zone_detail in comparison_result['zone_details'].items():
            if zone_detail['boundary'] < 70:
                boundary_diff = test_result.zones[zone_name].boundary - std_result.zones[zone_name].boundary
                self.reasons.append({
                    'category': 'BOUNDARY',
                    'severity': 100 - zone_detail['boundary'],
                    'zone': zone_name,
                    'message': f"Zone {zone_name} ê²½ê³„ ìœ„ì¹˜ {boundary_diff:+.1f}px ì´íƒˆ (ê¸°ì¤€ ëŒ€ë¹„ {abs(boundary_diff)/std_result.zones[zone_name].boundary*100:.1f}%)",
                    'action': f"{'ì™¸ê³½' if boundary_diff > 0 else 'ë‚´ì¸¡'} ë°©í–¥ ì¡°ì • í•„ìš”"
                })

        # 2. Zone ìƒ‰ìƒ ì°¨ì´
        for zone_name, zone_detail in comparison_result['zone_details'].items():
            if zone_detail['color'] < 70:
                delta_e = delta_e_cie2000(
                    test_result.zones[zone_name].mean_lab,
                    std_result.zones[zone_name].mean_lab
                )
                self.reasons.append({
                    'category': 'ZONE_COLOR',
                    'severity': delta_e * 10,
                    'zone': zone_name,
                    'message': f"Zone {zone_name} ìƒ‰ìƒ Î”E={delta_e:.2f} (ê¸°ì¤€: <3.0)",
                    'color_diff': self._format_color_diff(test_result.zones[zone_name].mean_lab, std_result.zones[zone_name].mean_lab),
                    'action': "ìƒ‰ìƒ ë†ë„ ì¡°ì • í•„ìš”"
                })

        # 3. Ink ê°œìˆ˜ ë¶ˆì¼ì¹˜
        if comparison_result['ink_details']['count_penalty'] > 0:
            test_n = test_result.ink_analysis.n_inks
            std_n = std_result.ink_analysis.n_inks
            self.reasons.append({
                'category': 'INK_COUNT',
                'severity': comparison_result['ink_details']['count_penalty'],
                'message': f"ì‰í¬ ê°œìˆ˜ ë¶ˆì¼ì¹˜: ê²€ì¶œ={test_n}ê°œ, ê¸°ì¤€={std_n}ê°œ",
                'action': f"{'ì‰í¬ ì¶”ê°€' if test_n < std_n else 'ì‰í¬ ì œê±°/í˜¼í•© í™•ì¸'} í•„ìš”"
            })

        # 4. Ink ìƒ‰ìƒ ì°¨ì´
        for idx, (test_idx, std_idx) in enumerate(comparison_result['ink_details']['matching']):
            delta_e = comparison_result['ink_details']['matched_delta_e'][idx]
            if delta_e > 3.0:
                self.reasons.append({
                    'category': 'INK_COLOR',
                    'severity': delta_e * 10,
                    'ink_index': idx,
                    'message': f"Ink #{idx+1} ìƒ‰ìƒ Î”E={delta_e:.2f} (ê¸°ì¤€: <3.0)",
                    'color_diff': self._format_color_diff(
                        test_result.ink_analysis.colors_lab[test_idx],
                        std_result.ink_analysis.colors_lab[std_idx]
                    ),
                    'action': "í•´ë‹¹ ì‰í¬ ë†ë„ ì¡°ì •"
                })

        # 5. Hotspot (êµ­ì†Œ ê²°í•¨)
        if 'hotspots' in comparison_result and len(comparison_result['hotspots']) > 0:
            for hotspot in comparison_result['hotspots'][:2]:  # ìµœëŒ€ 2ê°œ
                self.reasons.append({
                    'category': 'HOTSPOT',
                    'severity': hotspot['mean_delta_e'] * hotspot['area'] / 100,
                    'message': f"êµ­ì†Œ ìƒ‰ìƒ ì´ìƒ: ìœ„ì¹˜=({hotspot['centroid'][0]:.0f}, {hotspot['centroid'][1]:.0f}), ë©´ì ={hotspot['area']}px, í‰ê·  Î”E={hotspot['mean_delta_e']:.2f}",
                    'action': "í•´ë‹¹ ì˜ì—­ ìœ¡ì•ˆ ì ê²€ ë° ì¬ì‘ì—…"
                })

        # 6. 95% Î”E ì´ˆê³¼
        if 'percentiles' in comparison_result:
            p95 = comparison_result['percentiles']['p95']
            if p95 > 8.0:
                self.reasons.append({
                    'category': 'P95_DELTA_E',
                    'severity': p95 * 5,
                    'message': f"95% Î”E={p95:.2f} (ê¸°ì¤€: <8.0) - ìƒìœ„ 5% í”½ì…€ ìƒ‰ìƒ ì´ìƒ",
                    'action': "ì „ë°˜ì  ìƒ‰ìƒ í’ˆì§ˆ ê°œì„  í•„ìš”"
                })

        return self.reasons

    def _format_color_diff(self, test_lab, std_lab):
        """Lab ì°¨ì´ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ"""
        diff_L = test_lab[0] - std_lab[0]
        diff_a = test_lab[1] - std_lab[1]
        diff_b = test_lab[2] - std_lab[2]

        parts = []
        if abs(diff_L) > 2.0:
            parts.append(f"{'ë°ê¸°' if diff_L > 0 else 'ì–´ë‘ì›€'} {abs(diff_L):.1f}")
        if abs(diff_a) > 2.0:
            parts.append(f"{'ë¹¨ê°•' if diff_a > 0 else 'ì´ˆë¡'} {abs(diff_a):.1f}")
        if abs(diff_b) > 2.0:
            parts.append(f"{'ë…¸ë‘' if diff_b > 0 else 'íŒŒë‘'} {abs(diff_b):.1f}")

        return ', '.join(parts) if parts else 'ë¯¸ì„¸ ì°¨ì´'
```

#### 5.2.2 Top 3 Ranking
```python
def get_top_failure_reasons(self, n=3):
    """Severity ê¸°ì¤€ Top N ì¶”ì¶œ"""
    sorted_reasons = sorted(self.reasons, key=lambda x: x['severity'], reverse=True)
    return sorted_reasons[:n]
```

### 5.3 UI/Report í†µí•©
```python
def generate_failure_report(test_result, std_result, comparison_result):
    """ì‹¤íŒ¨ ë³´ê³ ì„œ ìë™ ìƒì„±"""
    extractor = FailureReasonExtractor()
    all_reasons = extractor.extract_all_reasons(test_result, std_result, comparison_result)
    top_reasons = extractor.get_top_failure_reasons(n=3)

    report = {
        'judgment': 'FAIL',
        'total_score': comparison_result['total_score'],
        'top_3_reasons': [
            {
                'rank': idx + 1,
                'category': reason['category'],
                'message': reason['message'],
                'action': reason['action'],
                'severity': reason['severity']
            }
            for idx, reason in enumerate(top_reasons)
        ],
        'all_reasons_count': len(all_reasons),
        'detailed_reasons': all_reasons
    }

    return report
```

### 5.4 Output Example
```json
{
  "judgment": "FAIL",
  "total_score": 65.3,
  "top_3_reasons": [
    {
      "rank": 1,
      "category": "ZONE_COLOR",
      "message": "Zone B ìƒ‰ìƒ Î”E=8.5 (ê¸°ì¤€: <3.0)",
      "color_diff": "ë¹¨ê°• 4.2, ë…¸ë‘ 3.8",
      "action": "ìƒ‰ìƒ ë†ë„ ì¡°ì • í•„ìš”",
      "severity": 85
    },
    {
      "rank": 2,
      "category": "BOUNDARY",
      "message": "Zone A ê²½ê³„ ìœ„ì¹˜ +12.3px ì´íƒˆ (ê¸°ì¤€ ëŒ€ë¹„ 4.8%)",
      "action": "ì™¸ê³½ ë°©í–¥ ì¡°ì • í•„ìš”",
      "severity": 61
    },
    {
      "rank": 3,
      "category": "INK_COUNT",
      "message": "ì‰í¬ ê°œìˆ˜ ë¶ˆì¼ì¹˜: ê²€ì¶œ=2ê°œ, ê¸°ì¤€=3ê°œ",
      "action": "ì‰í¬ ì¶”ê°€ í•„ìš”",
      "severity": 30
    }
  ]
}
```

---

## 6. Performance & Stability (ì„±ëŠ¥ ë° ì•ˆì •ì„±)

### 6.1 Profile Caching
```python
from functools import lru_cache
import hashlib

class STDProfileCache:
    def __init__(self, max_size=100):
        self.cache = {}
        self.max_size = max_size
        self.access_count = {}

    def get_cache_key(self, sku_code, version):
        """ìºì‹œ í‚¤ ìƒì„±"""
        return f"{sku_code}_{version}"

    @lru_cache(maxsize=100)
    def load_std_profile(self, sku_code, version):
        """STD í”„ë¡œíŒŒì¼ ë¡œë“œ (ìºì‹±)"""
        key = self.get_cache_key(sku_code, version)

        if key in self.cache:
            self.access_count[key] += 1
            return self.cache[key]

        # DBì—ì„œ ë¡œë“œ
        profile = self._load_from_db(sku_code, version)

        # ìºì‹œ ì €ì¥
        if len(self.cache) >= self.max_size:
            self._evict_lru()

        self.cache[key] = profile
        self.access_count[key] = 1

        return profile

    def _evict_lru(self):
        """LRU ì œê±°"""
        lru_key = min(self.access_count, key=self.access_count.get)
        del self.cache[lru_key]
        del self.access_count[lru_key]
```

### 6.2 Profile Normalization
```python
def normalize_radial_profile(profile, method='zscore'):
    """í”„ë¡œíŒŒì¼ ì •ê·œí™”"""
    if method == 'zscore':
        # Z-score normalization
        mean = np.mean(profile)
        std = np.std(profile)
        return (profile - mean) / std

    elif method == 'minmax':
        # Min-Max normalization
        min_val = np.min(profile)
        max_val = np.max(profile)
        return (profile - min_val) / (max_val - min_val)

    elif method == 'robust':
        # Robust normalization (IQR)
        q1 = np.percentile(profile, 25)
        q3 = np.percentile(profile, 75)
        iqr = q3 - q1
        median = np.median(profile)
        return (profile - median) / iqr
```

### 6.3 Fail-Safe Design
```python
class RobustComparisonEngine:
    def compare_with_failsafe(self, test_result, std_result):
        """ì‹¤íŒ¨ ë°©ì§€ ì„¤ê³„"""
        try:
            # 1. ì…ë ¥ ê²€ì¦
            self._validate_inputs(test_result, std_result)

            # 2. ë¹„êµ ìˆ˜í–‰
            comparison = self._perform_comparison(test_result, std_result)

            # 3. ê²°ê³¼ ê²€ì¦
            self._validate_results(comparison)

            return comparison

        except LensDetectionFailure as e:
            # ë Œì¦ˆ ë¯¸ê²€ì¶œ â†’ RETAKE
            return {
                'status': 'RETAKE',
                'reason': 'ë Œì¦ˆ ê²€ì¶œ ì‹¤íŒ¨',
                'detail': str(e),
                'action': 'ì´ë¯¸ì§€ ì¬ì´¬ì˜ í•„ìš”'
            }

        except InsufficientDataError as e:
            # ë°ì´í„° ë¶€ì¡± â†’ RETAKE
            return {
                'status': 'RETAKE',
                'reason': 'ë¶„ì„ ë°ì´í„° ë¶€ì¡±',
                'detail': str(e),
                'action': 'ì´¬ì˜ ì¡°ê±´ ê°œì„  í›„ ì¬ì´¬ì˜'
            }

        except Exception as e:
            # ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ â†’ ìˆ˜ë™ ì ê²€
            logger.error(f"Unexpected error: {e}")
            return {
                'status': 'MANUAL_REVIEW',
                'reason': 'ì‹œìŠ¤í…œ ì˜¤ë¥˜',
                'detail': str(e),
                'action': 'ë‹´ë‹¹ì í™•ì¸ í•„ìš”'
            }

    def _validate_inputs(self, test_result, std_result):
        """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        # ë Œì¦ˆ ê²€ì¶œ í™•ì¸
        if not test_result.lens_detected:
            raise LensDetectionFailure("Test sample: Lens not detected")

        # Zone ê°œìˆ˜ í™•ì¸
        if len(test_result.zones) != len(std_result.zones):
            raise InsufficientDataError(f"Zone count mismatch: {len(test_result.zones)} vs {len(std_result.zones)}")

        # Radial Profile ê¸¸ì´ í™•ì¸
        if len(test_result.radial_profile) < 100:
            raise InsufficientDataError("Radial profile too short")
```

### 6.4 Confidence Score
```python
def calculate_confidence_score(test_result, comparison_result):
    """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (íŒì •ê³¼ ë¶„ë¦¬)"""
    confidence_factors = []

    # 1. Lens Detection Quality
    if test_result.lens_detection_score > 0.95:
        confidence_factors.append(100)
    elif test_result.lens_detection_score > 0.80:
        confidence_factors.append(70)
    else:
        confidence_factors.append(30)

    # 2. Zone Segmentation Quality
    zone_confidence = np.mean([z.segmentation_confidence for z in test_result.zones.values()])
    confidence_factors.append(zone_confidence * 100)

    # 3. Alignment Quality
    if 'alignment_quality' in comparison_result:
        alignment_conf = comparison_result['alignment_quality']['correlation'] * 100
        confidence_factors.append(alignment_conf)

    # 4. Data Completeness
    completeness = (
        (1.0 if test_result.ink_analysis else 0.5) * 50 +
        (1.0 if len(test_result.zones) == 3 else 0.5) * 50
    )
    confidence_factors.append(completeness)

    overall_confidence = np.mean(confidence_factors)

    return {
        'confidence': overall_confidence,
        'factors': confidence_factors,
        'recommendation': 'HIGH' if overall_confidence > 80 else 'MEDIUM' if overall_confidence > 60 else 'RETAKE'
    }
```

---

## 7. Phenomenological Classification (í˜„ìƒí•™ì  ë¶„ë¥˜)

### 7.1 ê²°í•¨ ìœ í˜• ë¶„ë¥˜ ì²´ê³„

#### 7.1.1 Taxonomy
```python
DEFECT_TAXONOMY = {
    'COLOR_DEFECTS': {
        'UNDERDOSE': 'ìƒ‰ìƒ ë†ë„ ë¶€ì¡±í˜•',  # Î”E > 0, Lâ†‘
        'OVERDOSE': 'ìƒ‰ìƒ ë†ë„ ê³¼ë‹¤í˜•',   # Î”E > 0, Lâ†“
        'HUE_SHIFT': 'ìƒ‰ì¡° ë³€í™”í˜•',       # Î”a or Î”b > Î”L
        'FADING': 'ì „ë°˜ì  íƒˆìƒ‰í˜•',        # All zones Î”Eâ†‘
        'LOCALIZED': 'êµ­ì†Œ ë³€ìƒ‰í˜•'        # Hotspot ì¡´ì¬
    },
    'STRUCTURE_DEFECTS': {
        'BOUNDARY_BLUR': 'ê²½ê³„ íë¦¼í˜•',   # Transition width > std
        'MISALIGNMENT': 'ì¤‘ì‹¬ ì¹˜ìš°ì¹¨í˜•',  # Shift > 5%
        'ZONE_EXPANSION': 'ì˜ì—­ í™•ì¥í˜•',  # Zone area > std
        'ZONE_SHRINKAGE': 'ì˜ì—­ ì¶•ì†Œí˜•'   # Zone area < std
    },
    'INK_DEFECTS': {
        'INK_MISSING': 'ì‰í¬ ëˆ„ë½í˜•',     # n_inks < std
        'INK_EXTRA': 'ì‰í¬ ì¶”ê°€í˜•',       # n_inks > std
        'INK_MIXING': 'ì‰í¬ í˜¼í•© ì´ìƒí˜•'  # Mixing detected
    },
    'COMPOSITE': {
        'MULTI_DEFECT': 'ë³µí•© ê²°í•¨í˜•'     # 2ê°œ ì´ìƒ ë™ì‹œ ë°œìƒ
    }
}
```

#### 7.1.2 Classification Algorithm
```python
class DefectClassifier:
    def classify(self, test_result, std_result, comparison_result):
        """ê²°í•¨ ìœ í˜• ë¶„ë¥˜"""
        defects = []

        # 1. ìƒ‰ìƒ ê²°í•¨ ë¶„ë¥˜
        for zone_name, zone_detail in comparison_result['zone_details'].items():
            if zone_detail['color'] < 70:
                defect_type = self._classify_color_defect(
                    test_result.zones[zone_name],
                    std_result.zones[zone_name]
                )
                defects.append({
                    'category': 'COLOR_DEFECTS',
                    'type': defect_type,
                    'zone': zone_name,
                    'severity': 100 - zone_detail['color']
                })

        # 2. êµ¬ì¡° ê²°í•¨ ë¶„ë¥˜
        if comparison_result.get('alignment_quality', {}).get('shift', 0) > 5.0:
            defects.append({
                'category': 'STRUCTURE_DEFECTS',
                'type': 'MISALIGNMENT',
                'severity': comparison_result['alignment_quality']['shift'] * 10
            })

        for zone_name, zone_detail in comparison_result['zone_details'].items():
            if zone_detail['boundary'] < 70:
                defects.append({
                    'category': 'STRUCTURE_DEFECTS',
                    'type': 'BOUNDARY_BLUR',
                    'zone': zone_name,
                    'severity': 100 - zone_detail['boundary']
                })

        # 3. ì‰í¬ ê²°í•¨ ë¶„ë¥˜
        if comparison_result['ink_details']['count_penalty'] > 0:
            test_n = test_result.ink_analysis.n_inks
            std_n = std_result.ink_analysis.n_inks

            if test_n < std_n:
                defect_type = 'INK_MISSING'
            else:
                defect_type = 'INK_EXTRA'

            defects.append({
                'category': 'INK_DEFECTS',
                'type': defect_type,
                'severity': comparison_result['ink_details']['count_penalty']
            })

        # 4. ë³µí•© ê²°í•¨ íŒë‹¨
        if len(defects) >= 2:
            defects.append({
                'category': 'COMPOSITE',
                'type': 'MULTI_DEFECT',
                'component_count': len(defects),
                'severity': np.mean([d['severity'] for d in defects])
            })

        return defects

    def _classify_color_defect(self, test_zone, std_zone):
        """ìƒ‰ìƒ ê²°í•¨ ì„¸ë¶€ ë¶„ë¥˜"""
        diff_L = test_zone.mean_lab[0] - std_zone.mean_lab[0]
        diff_a = test_zone.mean_lab[1] - std_zone.mean_lab[1]
        diff_b = test_zone.mean_lab[2] - std_zone.mean_lab[2]

        # Lightness ë³€í™”ê°€ ì£¼ ì›ì¸
        if abs(diff_L) > abs(diff_a) and abs(diff_L) > abs(diff_b):
            if diff_L > 0:
                return 'UNDERDOSE'  # ë°ì•„ì§ = ë†ë„ ë¶€ì¡±
            else:
                return 'OVERDOSE'   # ì–´ë‘ì›Œì§ = ë†ë„ ê³¼ë‹¤

        # Hue ë³€í™”ê°€ ì£¼ ì›ì¸
        else:
            return 'HUE_SHIFT'
```

### 7.2 ML Training Data Preparation
```python
def export_for_ml_training(comparison_results, defect_classifications):
    """ML í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„"""
    training_data = []

    for result, classification in zip(comparison_results, defect_classifications):
        # Feature Vector êµ¬ì„±
        features = {
            # Structure features
            'boundary_A_diff': result['zone_details']['A']['boundary'],
            'boundary_B_diff': result['zone_details']['B']['boundary'],
            'boundary_C_diff': result['zone_details']['C']['boundary'],
            'alignment_shift': result.get('alignment_quality', {}).get('shift', 0),

            # Color features
            'zone_A_delta_e': result['zone_details']['A']['color'],
            'zone_B_delta_e': result['zone_details']['B']['color'],
            'zone_C_delta_e': result['zone_details']['C']['color'],
            'p95_delta_e': result.get('percentiles', {}).get('p95', 0),

            # Ink features
            'ink_count_diff': result['ink_details']['count_penalty'],
            'avg_ink_delta_e': np.mean(result['ink_details']['matched_delta_e']),

            # Hotspot features
            'hotspot_count': len(result.get('hotspots', [])),
            'max_hotspot_area': max([h['area'] for h in result.get('hotspots', [])], default=0)
        }

        # Label: Primary defect type
        primary_defect = max(classification, key=lambda x: x['severity'])
        label = f"{primary_defect['category']}_{primary_defect['type']}"

        training_data.append({
            'features': features,
            'label': label,
            'severity': primary_defect['severity']
        })

    return training_data
```

---

## 8. Updated System Architecture

### 8.1 Enhanced Comparison Pipeline
```
Input: Test Image + STD Model
    â†“
[Phase 1] Analysis
    - Lens Detection
    - Zone Segmentation
    - Ink Estimation
    - Radial Profiling
    â†“
[Phase 2] Statistical Comparison
    - Load STD Statistical Model (cached)
    - Profile Normalization
    â†“
[Phase 3] Elastic Alignment
    - Anchor Zone Identification
    - Circular Shift Alignment
    - DTW Fine-Tuning
    - Alignment Quality Check
    â†“
[Phase 4] Dual Scoring
    - Zone-Based Score (structure)
    - Ink-Based Score (color)
    - Worst-Case Metrics (p95, hotspots)
    â†“
[Phase 5] Judgment & Explainability
    - Combined Score Calculation
    - Pass/Fail Decision
    - Failure Reason Extraction (Top 3)
    - Defect Classification
    - Confidence Score
    â†“
[Phase 6] Output
    - Judgment: PASS / FAIL / RETAKE / MANUAL_REVIEW
    - Score: 0-100
    - Confidence: 0-100
    - Reasons: Top 3 with actions
    - Defect Types: Phenomenological classification
    - Visualizations: Delta E map, Hotspot overlay
```

### 8.2 Updated DB Schema
```sql
-- STD í†µê³„ ëª¨ë¸ (Multiple samples)
CREATE TABLE std_models (
    id SERIAL PRIMARY KEY,
    sku_code VARCHAR(50) NOT NULL,
    version VARCHAR(20) NOT NULL,
    n_samples INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    approved_by VARCHAR(100),
    approved_at TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE,
    UNIQUE(sku_code, version)
);

CREATE TABLE std_samples (
    id SERIAL PRIMARY KEY,
    std_model_id INTEGER REFERENCES std_models(id) ON DELETE CASCADE,
    sample_index INTEGER,
    image_path VARCHAR(500),
    analysis_result JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE std_statistics (
    id SERIAL PRIMARY KEY,
    std_model_id INTEGER REFERENCES std_models(id) ON DELETE CASCADE,
    zone_name VARCHAR(10),

    -- Color statistics
    mean_L FLOAT,
    std_L FLOAT,
    mean_a FLOAT,
    std_a FLOAT,
    mean_b FLOAT,
    std_b FLOAT,

    -- Structure statistics
    mean_position FLOAT,
    std_position FLOAT,
    percentile_5 FLOAT,
    percentile_95 FLOAT,

    -- Detailed stats
    detailed_stats JSONB
);

-- Comparison results (enhanced)
CREATE TABLE comparison_results (
    id SERIAL PRIMARY KEY,
    test_sample_id INTEGER REFERENCES test_samples(id),
    std_model_id INTEGER REFERENCES std_models(id),

    -- Scores
    total_score FLOAT,
    zone_score FLOAT,
    ink_score FLOAT,
    confidence_score FLOAT,

    -- Judgment
    judgment VARCHAR(20),  -- PASS, FAIL, RETAKE, MANUAL_REVIEW

    -- Explainability
    top_failure_reasons JSONB,  -- Top 3 reasons
    defect_classifications JSONB,  -- Phenomenological types

    -- Detailed results
    zone_details JSONB,
    ink_details JSONB,
    alignment_details JSONB,
    worst_case_metrics JSONB,

    -- Metadata
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processing_time_ms INTEGER
);

CREATE INDEX idx_comparison_judgment ON comparison_results(judgment);
CREATE INDEX idx_comparison_score ON comparison_results(total_score);
CREATE INDEX idx_std_statistics_model ON std_statistics(std_model_id, zone_name);
```

---

## 9. Implementation Roadmap (Updated)

### Phase 0: Technical Foundation (Week 1-2)
- [ ] **Week 1.0**: Algorithm Benchmarking
  - DTW vs FastDTW performance test (100 samples)
  - Alignment algorithm prototyping
  - Worst-case metrics validation
- [ ] **Week 1.5**: DB Schema Migration
  - Implement `std_models`, `std_samples`, `std_statistics`
  - Migration scripts
  - SQLAlchemy models

### Phase 1: STD Statistical Model (Week 3-4)
- [ ] **Week 3**: Multi-Sample Registration
  - UI for uploading 5-10 STD samples
  - Batch analysis pipeline
  - Statistical calculation engine
- [ ] **Week 4**: Acceptance Criteria Derivation
  - Auto-calculate thresholds from distribution
  - Upper/Middle/Lower limit configuration

### Phase 2: Enhanced Comparison Engine (Week 5-8)
- [ ] **Week 5-6**: Elastic Alignment
  - Anchor zone detection
  - Circular shift + DTW implementation
  - Alignment quality metrics
- [ ] **Week 7**: Dual Scoring + Worst-Case Metrics
  - Zone-based + Ink-based scoring
  - Percentile Î”E calculation
  - Hotspot detection
- [ ] **Week 8**: Explainability Layer
  - Failure reason extraction
  - Top 3 ranking
  - Defect classification

### Phase 3: Production Features (Week 9-10)
- [ ] **Week 9**: Fail-Safe & Confidence
  - Profile caching
  - RETAKE logic
  - Confidence score separation
- [ ] **Week 10**: UI/Dashboard
  - Comparison results visualization
  - Delta E heatmap
  - Explainability report UI

### Phase 4: Advanced Analytics (v2.0)
- [ ] ML model training (defect classification)
- [ ] Process control charts
- [ ] Automated adjustment recommendations

---

## 10. Success Metrics

### Technical Metrics
- **Performance**: avg < 1.0s, p99 < 3.0s
- **Accuracy**: 95% agreement with manual inspection
- **False Negative Rate**: < 2% (NGë¥¼ OKë¡œ íŒì •)
- **False Positive Rate**: < 5% (OKë¥¼ NGë¡œ íŒì •)

### Operational Metrics
- **Explainability**: 100% FAIL cases have Top 3 reasons
- **Actionability**: 90% reasons include specific actions
- **Confidence**: 80% results have confidence > 80%

### Business Metrics
- **Inspection Time**: 3ë¶„ â†’ 30ì´ˆ (6ë°° í–¥ìƒ)
- **Manual Review Rate**: < 3% (MANUAL_REVIEW + RETAKE)
- **Defect Detection Recall**: > 98%

---

## 11. Risk Mitigation

### Technical Risks
1. **Alignment ì‹¤íŒ¨**: Correlation < 0.8
   - Mitigation: Fail-safe â†’ RETAKE
2. **Hotspot ê³¼ê²€ì¶œ**: False alarm
   - Mitigation: min_area threshold tuning
3. **DB ì„±ëŠ¥ ì €í•˜**: í†µê³„ í…Œì´ë¸” ì¡°ì¸ ë¹„ìš©
   - Mitigation: ì¸ë±ìŠ¤ ìµœì í™”, ìºì‹±

### Operational Risks
1. **STD ìƒ˜í”Œ í’ˆì§ˆ ì €í•˜**: ë¶ˆëŸ‰ ìƒ˜í”Œ í¬í•¨
   - Mitigation: Approval workflow, outlier detection
2. **ì„¤ëª… ê³¼ì‰**: Top 3 reasons í•´ì„ ì–´ë ¤ì›€
   - Mitigation: ì‚¬ìš©ì êµìœ¡, UI ê°œì„ 

---

## 12. Next Steps

1. **ì•Œê³ ë¦¬ì¦˜ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±** (ì¦‰ì‹œ ì°©ìˆ˜)
   - `tools/benchmark_alignment.py`
   - DTW vs FastDTW ë¹„êµ
   - 100 ìƒ˜í”Œ í…ŒìŠ¤íŠ¸

2. **DB ìŠ¤í‚¤ë§ˆ ê°œì„  êµ¬í˜„**
   - `src/models/std_models.py` (SQLAlchemy)
   - Alembic migration scripts

3. **í†µê³„ ëª¨ë¸ ì—”ì§„ êµ¬í˜„**
   - `src/services/std_statistical_model.py`
   - Multi-sample aggregation
   - Acceptance criteria derivation

4. **Elastic Alignment í”„ë¡œí† íƒ€ì…**
   - `src/core/alignment.py`
   - Anchor zone detection
   - DTW fine-tuning

---

**ì‘ì„±ì**: Claude Sonnet 4.5
**ê²€í†  í•„ìš”**: ì•Œê³ ë¦¬ì¦˜ íƒ€ë‹¹ì„±, ì„±ëŠ¥ ëª©í‘œ, ì¼ì • í˜„ì‹¤ì„±
**ìŠ¹ì¸ í›„ ì‘ì—…**: Phase 0 (Week 1) ì°©ìˆ˜
